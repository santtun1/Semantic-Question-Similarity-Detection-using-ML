{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwYWg08XQ6fk",
        "outputId": "079380e5-22da-460c-8f8b-1c513d89ad50",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.1.0)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.11.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required modules\n",
        "!pip install easyocr\n",
        "!pip install PyMuPDF\n",
        "!pip install fpdf\n",
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrzgMt6jRO-b",
        "outputId": "1097330a-eaf0-45f6-c934-8f4b44197a62",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwd587P6R2Z_",
        "outputId": "ed7a089c-bced-493b-a4f4-90b3aa1dfe24",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spellchecker in /usr/local/lib/python3.11/dist-packages (0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spellchecker) (79.0.1)\n",
            "Requirement already satisfied: inexactsearch in /usr/local/lib/python3.11/dist-packages (from spellchecker) (1.0.2)\n",
            "Requirement already satisfied: soundex>=1.0 in /usr/local/lib/python3.11/dist-packages (from inexactsearch->spellchecker) (1.1.3)\n",
            "Requirement already satisfied: silpa-common>=0.3 in /usr/local/lib/python3.11/dist-packages (from inexactsearch->spellchecker) (0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjcfplicR-q9",
        "outputId": "482b17cc-61b5-4938-ce82-e7b6d2d544ae",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (79.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwO6fGB-SFfn",
        "outputId": "a0b3a3ee-6241-4aad-f197-53016396761f",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCkHRK89SLBD",
        "outputId": "4b51c07b-79c5-407d-a276-8dc9b747cec9",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.11/dist-packages (0.7.5)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.11/dist-packages (from textstat) (0.17.2)\n",
            "Requirement already satisfied: cmudict in /usr/local/lib/python3.11/dist-packages (from textstat) (1.0.32)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (79.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiWqfpA8SNma",
        "outputId": "77460280-b27c-4934-e423-61a62ad60e52",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN7qvENvSQYQ",
        "outputId": "cd0a414a-5934-41ca-a2ad-927ae2f2a136",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (79.0.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install indexer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_cjniyDSVeN",
        "outputId": "8b97c095-1280-4809-dd5c-fa976b7b3b5f",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indexer\n",
            "  Downloading indexer-0.6.2.tar.gz (14 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import easyocr\n",
        "from fpdf import FPDF\n",
        "import fitz  # PyMuPDF\n",
        "from google.colab import files\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from spellchecker import SpellChecker\n",
        "from textstat import textstat\n",
        "\n",
        "# Download necessary NLTK resources with explicit download path\n",
        "print(\"üì¶ Downloading NLTK resources...\")\n",
        "nltk.download('punkt')  # Just punkt, not punkt_tab\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "print(\"‚úÖ NLTK resources downloaded\")\n",
        "\n",
        "# Make sure we're using the right tokenizers\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    print(\"‚úÖ spaCy model loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è spaCy model error: {e}\")\n",
        "    print(\"Installing spaCy model...\")\n",
        "    import os\n",
        "    os.system('python -m spacy download en_core_web_sm')\n",
        "    try:\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        print(\"‚úÖ spaCy model loaded after installation\")\n",
        "    except:\n",
        "        print(\"‚ùå Could not load spaCy model\")\n",
        "        # Fallback option\n",
        "        nlp = None\n",
        "\n",
        "# Initialize spellchecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# --- TEXT EXTRACTION FUNCTIONS ---\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    \"\"\"Extract text from an image using EasyOCR\"\"\"\n",
        "    reader = easyocr.Reader(['en'], gpu=False)\n",
        "    result = reader.readtext(image_path, detail=0, paragraph=True)\n",
        "    return \"\\n\".join(result)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF using PyMuPDF\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# --- NLP PROCESSING FUNCTIONS ---\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_ocr_text(text):\n",
        "    \"\"\"Clean OCR-extracted text by fixing common character errors and formatting issues.\"\"\"\n",
        "\n",
        "    # --- STEP 1: Basic OCR format fixes ---\n",
        "    text = re.sub(r'([A-Za-z])_([A-Za-z])', r'\\1 \\2', text)  # Underscores between letters ‚Üí space\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)          # Lowercase followed by uppercase ‚Üí space\n",
        "\n",
        "    # --- STEP 2: Common static replacements ---\n",
        "    replacements = {\n",
        "        'Fo': 'to',\n",
        "        'Fhe': 'the',\n",
        "        'Fhis': 'this',\n",
        "        'Fhat': 'that',\n",
        "        'Falking': 'talking',\n",
        "        'Fearning': 'learning',\n",
        "        'Fhings': 'things',\n",
        "        'Talso': 'I also',\n",
        "        'Fhink': 'think',\n",
        "        'lfe': 'life',\n",
        "        '&th': '8th',\n",
        "        'Zgo': 'I go',\n",
        "        '1 am': 'I am',\n",
        "\n",
        "        # Additional likely OCR-based typos\n",
        "        'Teh': 'The',\n",
        "        'Ths': 'This',\n",
        "        'Fere': 'There',\n",
        "        'Fom': 'From',\n",
        "        'Fime': 'Time',\n",
        "        'Foday': 'Today',\n",
        "        'Fey': 'They',\n",
        "        'Tey': 'They',\n",
        "        'Ferefore': 'Therefore',\n",
        "        'Frue': 'True',\n",
        "        'Fust': 'Just',\n",
        "        'l': 'I',  # lowercase L often used for uppercase I\n",
        "        'i m': 'I am',\n",
        "        'ive': 'I have',\n",
        "        'i ve': 'I have',\n",
        "        'thw': 'the',\n",
        "        'thid': 'this',\n",
        "        'thar': 'that',\n",
        "        'woukd': 'would',\n",
        "        'coud': 'could',\n",
        "        'shoukd': 'should',\n",
        "        'Sth': '8th',\n",
        "        'gth': '9th',\n",
        "        'Oth': '10th'\n",
        "    }\n",
        "\n",
        "    for wrong, right in replacements.items():\n",
        "        text = text.replace(wrong, right)\n",
        "\n",
        "    # --- STEP 3: Regex-based dynamic corrections ---\n",
        "    # Replace 'F' + vowel-starting word ‚Üí 'Th'\n",
        "    text = re.sub(r'\\bF(?=[aeiouAEIOU])', 'Th', text)\n",
        "\n",
        "    # Replace '1' at word beginning with 'I' (common OCR confusion)\n",
        "    text = re.sub(r'\\b1(?=\\w+)', 'I', text)\n",
        "\n",
        "    # Replace standalone lowercase l with uppercase I\n",
        "    text = re.sub(r'\\bl\\b', 'I', text)\n",
        "\n",
        "    # Fix weird ordinal numbers (e.g., &th)\n",
        "    text = re.sub(r'&th', '8th', text)\n",
        "\n",
        "    # Fix \"Z\" at start of word, usually meant to be \"I\"\n",
        "    text = re.sub(r'\\bZ(?=\\w+)', 'I', text)\n",
        "\n",
        "    # --- STEP 4: Whitespace cleanup ---\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def correct_spelling(text):\n",
        "    \"\"\"Correct spelling errors in the text\"\"\"\n",
        "    # Simple tokenization using Python's split to avoid NLTK issues\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # Only check words with letters (not numbers or punctuation alone)\n",
        "        if re.search('[a-zA-Z]', word):\n",
        "            # Remove punctuation attached to the word for spell checking\n",
        "            clean_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "\n",
        "            if clean_word and clean_word.lower() not in ['i', 'a', 'an', 'the', 'and', 'or', 'but', 'to', 'for', 'in', 'on', 'at', 'by']:\n",
        "                misspelled = spell.unknown([clean_word])\n",
        "                if misspelled:\n",
        "                    # Get the most likely correction\n",
        "                    correction = spell.correction(clean_word)\n",
        "                    if correction:\n",
        "                        # Replace just the word part, maintaining original punctuation\n",
        "                        corrected_word = word.replace(clean_word, correction)\n",
        "                        corrected_words.append(corrected_word)\n",
        "                    else:\n",
        "                        corrected_words.append(word)\n",
        "                else:\n",
        "                    corrected_words.append(word)\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "\n",
        "    return ' '.join(corrected_words)\n",
        "\n",
        "def fix_grammar_with_spacy(text):\n",
        "    \"\"\"Use spaCy for basic grammar correction\"\"\"\n",
        "    if nlp is None:\n",
        "        # Fallback basic correction if spaCy is not available\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        corrected_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if sentence:\n",
        "                # Capitalize first letter\n",
        "                if sentence[0].isalpha() and not sentence[0].isupper():\n",
        "                    sentence = sentence[0].upper() + sentence[1:]\n",
        "\n",
        "                # Add period if missing ending punctuation\n",
        "                if sentence[-1] not in ['.', '!', '?']:\n",
        "                    sentence += '.'\n",
        "\n",
        "                corrected_sentences.append(sentence)\n",
        "\n",
        "        return ' '.join(corrected_sentences)\n",
        "\n",
        "    try:\n",
        "        doc = nlp(text)\n",
        "        sentences = []\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            # Convert to string and capitalize first letter\n",
        "            sentence = sent.text.strip()\n",
        "            if sentence:\n",
        "                if sentence[0].isalpha() and not sentence[0].isupper():\n",
        "                    sentence = sentence[0].upper() + sentence[1:]\n",
        "\n",
        "                # Make sure sentence ends with punctuation\n",
        "                if sentence[-1] not in ['.', '!', '?']:\n",
        "                    sentence += '.'\n",
        "\n",
        "                sentences.append(sentence)\n",
        "\n",
        "        return ' '.join(sentences)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Grammar correction error: {e}\")\n",
        "        return text  # Return original text if there's an error\n",
        "\n",
        "def analyze_text_complexity(text):\n",
        "    \"\"\"Analyze readability of the text\"\"\"\n",
        "    if len(text.strip()) == 0:\n",
        "        return \"No text to analyze\"\n",
        "\n",
        "    try:\n",
        "        results = {\n",
        "            \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
        "            \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),\n",
        "            \"automated_readability_index\": textstat.automated_readability_index(text)\n",
        "        }\n",
        "\n",
        "        analysis = f\"Readability Analysis:\\n\"\n",
        "        analysis += f\"- Flesch Reading Ease: {results['flesch_reading_ease']:.1f}/100 \"\n",
        "\n",
        "        if results['flesch_reading_ease'] > 90:\n",
        "            analysis += \"(Very Easy to Read)\\n\"\n",
        "        elif results['flesch_reading_ease'] > 80:\n",
        "            analysis += \"(Easy to Read)\\n\"\n",
        "        elif results['flesch_reading_ease'] > 70:\n",
        "            analysis += \"(Fairly Easy to Read)\\n\"\n",
        "        elif results['flesch_reading_ease'] > 60:\n",
        "            analysis += \"(Standard/Plain English)\\n\"\n",
        "        elif results['flesch_reading_ease'] > 50:\n",
        "            analysis += \"(Fairly Difficult to Read)\\n\"\n",
        "        elif results['flesch_reading_ease'] > 30:\n",
        "            analysis += \"(Difficult to Read)\\n\"\n",
        "        else:\n",
        "            analysis += \"(Very Difficult to Read)\\n\"\n",
        "\n",
        "        analysis += f\"- Grade Level: {results['flesch_kincaid_grade']:.1f}\\n\"\n",
        "        analysis += f\"- Automated Readability Index: {results['automated_readability_index']:.1f}\"\n",
        "\n",
        "        return analysis\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Readability analysis error: {e}\")\n",
        "        return \"Unable to analyze text complexity\"\n",
        "\n",
        "def extract_key_entities(text):\n",
        "    \"\"\"Extract named entities from the text using spaCy\"\"\"\n",
        "    if nlp is None:\n",
        "        return \"Entity extraction not available (spaCy model not loaded)\"\n",
        "\n",
        "    try:\n",
        "        doc = nlp(text)\n",
        "        entities = {}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            entity_type = ent.label_\n",
        "            if entity_type not in entities:\n",
        "                entities[entity_type] = []\n",
        "            if ent.text not in entities[entity_type]:\n",
        "                entities[entity_type].append(ent.text)\n",
        "\n",
        "        if not entities:\n",
        "            return \"No named entities found in the text\"\n",
        "\n",
        "        result = \"Named Entities:\\n\"\n",
        "        for entity_type, items in entities.items():\n",
        "            result += f\"- {entity_type}: {', '.join(items)}\\n\"\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Entity extraction error: {e}\")\n",
        "        return \"Unable to extract entities\"\n",
        "\n",
        "def process_text_with_nlp(text):\n",
        "    \"\"\"Process text using various NLP techniques\"\"\"\n",
        "    print(\"1. Cleaning OCR artifacts...\")\n",
        "    # Initial cleaning of OCR artifacts\n",
        "    text = clean_ocr_text(text)\n",
        "\n",
        "    print(\"2. Correcting spelling...\")\n",
        "    # Correct spelling\n",
        "    text = correct_spelling(text)\n",
        "\n",
        "    print(\"3. Fixing grammar...\")\n",
        "    # Fix basic grammar issues\n",
        "    text = fix_grammar_with_spacy(text)\n",
        "\n",
        "    print(\"4. Analyzing text complexity...\")\n",
        "    # Additional analysis\n",
        "    complexity_analysis = analyze_text_complexity(text)\n",
        "\n",
        "    print(\"5. Extracting entities...\")\n",
        "    entity_analysis = extract_key_entities(text)\n",
        "\n",
        "    # Return processed text and analysis\n",
        "    return {\n",
        "        \"processed_text\": text,\n",
        "        \"complexity_analysis\": complexity_analysis,\n",
        "        \"entity_analysis\": entity_analysis\n",
        "    }\n",
        "\n",
        "# --- PDF GENERATION FUNCTION ---\n",
        "\n",
        "def create_pdf_from_text(text, analysis, output_pdf):\n",
        "    \"\"\"Create a PDF from extracted text and analysis\"\"\"\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "\n",
        "    # Add original processed text\n",
        "    pdf.set_font(\"Arial\", 'B', size=14)\n",
        "    pdf.cell(0, 10, \"Processed Text\", ln=True)\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.multi_cell(0, 10, text)\n",
        "\n",
        "    # Add analysis section\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", 'B', size=14)\n",
        "    pdf.cell(0, 10, \"Text Analysis\", ln=True)\n",
        "\n",
        "    pdf.set_font(\"Arial\", 'B', size=12)\n",
        "    pdf.cell(0, 10, \"Complexity Analysis\", ln=True)\n",
        "    pdf.set_font(\"Arial\", size=11)\n",
        "    pdf.multi_cell(0, 8, analysis[\"complexity_analysis\"])\n",
        "\n",
        "    pdf.set_font(\"Arial\", 'B', size=12)\n",
        "    pdf.cell(0, 10, \"Entity Analysis\", ln=True)\n",
        "    pdf.set_font(\"Arial\", size=11)\n",
        "    pdf.multi_cell(0, 8, analysis[\"entity_analysis\"])\n",
        "\n",
        "    # Add AI suggestions if available\n",
        "    if \"ai_suggestions\" in analysis:\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", 'B', size=14)\n",
        "        pdf.cell(0, 10, \"AI Improvement Suggestions\", ln=True)\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        pdf.multi_cell(0, 10, analysis[\"ai_suggestions\"])\n",
        "\n",
        "    pdf.output(output_pdf)\n",
        "\n",
        "# --- GEMINI IMPROVEMENT SUGGESTIONS ---\n",
        "def suggest_improvements_with_gemini(extracted_text, api_key):\n",
        "    \"\"\"\n",
        "    Prompt the user to enter a custom instruction, then use Gemini API\n",
        "    to generate suggestions or improvements on the extracted text.\n",
        "    \"\"\"\n",
        "    # Ask user to enter their prompt\n",
        "    print(\"\\nüì• Please enter your instruction or prompt for Gemini (e.g., 'Fix grammar and summarize'):\")\n",
        "    user_prompt = input(\"üìù Your Prompt: \").strip()\n",
        "\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "        # Limit extracted text to avoid overload\n",
        "        trimmed_text = extracted_text[:1500]\n",
        "\n",
        "        # Create the full prompt\n",
        "        full_prompt = f\"\"\"{user_prompt}\n",
        "\n",
        "OCR-extracted text:\n",
        "{trimmed_text}\n",
        "\"\"\"\n",
        "\n",
        "        # Generate content\n",
        "        response = model.generate_content(full_prompt)\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Gemini API Error: {e}\")\n",
        "        return f\"Unable to generate suggestions using Gemini. Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# --- MAIN PROCESSING FUNCTION ---\n",
        "\n",
        "def process_document(file_path, file_type, output_pdf, api_key=None):\n",
        "    \"\"\"Extract text, process with NLP, and generate a PDF from it\"\"\"\n",
        "    # Extract raw text from document\n",
        "    if file_type == 'image':\n",
        "        extracted_text = extract_text_from_image(file_path)\n",
        "    elif file_type == 'pdf':\n",
        "        extracted_text = extract_text_from_pdf(file_path)\n",
        "    else:\n",
        "        extracted_text = \"\"\n",
        "        print(\"‚ùå Unsupported file type\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n=== üìù Raw Extracted Text ===\")\n",
        "    print(extracted_text[:500] + \"...\" if len(extracted_text) > 500 else extracted_text)\n",
        "\n",
        "    # Process text with NLP\n",
        "    print(\"\\n=== üß† Processing with NLP... ===\")\n",
        "    nlp_results = process_text_with_nlp(extracted_text)\n",
        "\n",
        "    processed_text = nlp_results[\"processed_text\"]\n",
        "    print(\"\\n=== üìù NLP Processed Text ===\")\n",
        "    print(processed_text[:500] + \"...\" if len(processed_text) > 500 else processed_text)\n",
        "\n",
        "    # Add complexity and entity analysis\n",
        "    print(\"\\n=== üìä Text Analysis ===\")\n",
        "    print(nlp_results[\"complexity_analysis\"])\n",
        "    print(\"\\n\" + nlp_results[\"entity_analysis\"])\n",
        "\n",
        "    analysis_results = nlp_results\n",
        "\n",
        "    # Get AI suggestions if API key is provided\n",
        "    if api_key:\n",
        "        print(\"\\n=== ü§ñ Requesting AI suggestions... ===\")\n",
        "        suggestions = suggest_improvements_with_gemini(processed_text, api_key)\n",
        "        analysis_results[\"ai_suggestions\"] = suggestions\n",
        "        print(\"\\n=== üí° AI Suggestions ===\")\n",
        "        print(suggestions)\n",
        "\n",
        "    # Create and save PDF\n",
        "    create_pdf_from_text(processed_text, analysis_results, output_pdf)\n",
        "\n",
        "    return {\n",
        "        \"raw_text\": extracted_text,\n",
        "        \"processed_text\": processed_text,\n",
        "        \"analysis\": analysis_results\n",
        "    }\n",
        "\n",
        "# --- INTERACTIVE EXECUTION ---\n",
        "\n",
        "# Install required packages if they're not already installed\n",
        "try:\n",
        "    import pkg_resources\n",
        "    required_packages = ['spacy', 'textstat', 'pyspellchecker']\n",
        "    installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "    missing = [pkg for pkg in required_packages if pkg.lower() not in installed]\n",
        "\n",
        "    if missing:\n",
        "        print(f\"üì¶ Installing missing packages: {', '.join(missing)}\")\n",
        "        import os\n",
        "        os.system(f\"pip install {' '.join(missing)}\")\n",
        "        print(\"‚úÖ Packages installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Package check error: {e}\")\n",
        "\n",
        "# Input Gemini API Key (optional)\n",
        "api_key = input(\"üîë Enter your Gemini API key (press Enter to skip AI suggestions): \").strip()\n",
        "\n",
        "if api_key:\n",
        "    print(\"‚úÖ API key received.\")\n",
        "else:\n",
        "    print(\"‚è© Skipping AI suggestions.\")\n",
        "\n",
        "print(\"\\nüì§ Upload a file (image or PDF) to process:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    file_extension = filename.split('.')[-1].lower()\n",
        "\n",
        "    if file_extension in ['jpg', 'jpeg', 'png', 'bmp']:\n",
        "        file_type = 'image'\n",
        "    elif file_extension in ['pdf']:\n",
        "        file_type = 'pdf'\n",
        "    else:\n",
        "        print(\"‚ùå Unsupported file format.\")\n",
        "        file_type = None\n",
        "\n",
        "    if file_type:\n",
        "        print(f\"\\nüìÑ Processing {filename}...\")\n",
        "        output_pdf = \"processed_document_with_nlp.pdf\"\n",
        "\n",
        "        # Process the document with all NLP features\n",
        "        results = process_document(filename, file_type, output_pdf, api_key if api_key else None)\n",
        "\n",
        "        if results:\n",
        "            print(\"\\n‚¨áÔ∏è Downloading PDF...\")\n",
        "            files.download(output_pdf)\n",
        "            print(\"\\n‚úÖ Processing complete!\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Processing failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hM1frwjsSvtK",
        "outputId": "bf3693e9-216d-4dcd-b67b-be74a04bdc1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Downloading NLTK resources...\n",
            "‚úÖ NLTK resources downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ spaCy model loaded\n",
            "üîë Enter your Gemini API key (press Enter to skip AI suggestions): AIzaSyDtws4ifcnac0tnj7Z6-rpdxYeviy6aOPA\n",
            "‚úÖ API key received.\n",
            "\n",
            "üì§ Upload a file (image or PDF) to process:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da92811d-347c-48b6-b69d-3576ee436b2d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da92811d-347c-48b6-b69d-3576ee436b2d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Using CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving p3.jpg to p3 (1).jpg\n",
            "\n",
            "üìÑ Processing p3 (1).jpg...\n",
            "\n",
            "=== üìù Raw Extracted Text ===\n",
            "  DEA# GB 05455616  LIC # 976269    MEDICAL CENTRE  New York; NY 91743, USA 824 14u Street NAME Jola Smitl AGE 34 ADDRESS 162 Example St, NT DATE 09-11-12   Betaloc I0O~3 -144L Bid  Dorzolamizvm I0 ~J +45 Bid Cinetizine 50 ~J 2 +4L, TID Oxprelol 50~a t45 QD e     1  Dc. Steve_JoLason signature OLABEL  1 REFILL 0(12 3 4 5 PRN WTXSNY PRES7OO 1\n",
            "\n",
            "=== üß† Processing with NLP... ===\n",
            "1. Cleaning OCR artifacts...\n",
            "2. Correcting spelling...\n",
            "3. Fixing grammar...\n",
            "4. Analyzing text complexity...\n",
            "5. Extracting entities...\n",
            "\n",
            "=== üìù NLP Processed Text ===\n",
            "Dead# go 05455616 lie # 976269 MEDICAL center New York; my 91743, us 824 you Street NAME join smite AGE 34 ADDRESS. I example St, it DATE 09-I1-I2 betaine I0O~3. -i'll. Bid DorzoIamizvm i ~J +45 Bid cimetidine 50 ~J 2 +al, did OxpreIoI 50~a the ad e 1 Dc. Steve Jo jason signature label 1 REFILL 0(I2 3 4 5 pin WTXSNY preston 1.\n",
            "\n",
            "=== üìä Text Analysis ===\n",
            "Readability Analysis:\n",
            "- Flesch Reading Ease: 80.6/100 (Easy to Read)\n",
            "- Grade Level: 6.0\n",
            "- Automated Readability Index: 6.0\n",
            "\n",
            "Named Entities:\n",
            "- DATE: 05455616, 91743, AGE 34, 50\n",
            "- MONEY: 976269\n",
            "- ORG: MEDICAL, WTXSNY\n",
            "- GPE: New York\n",
            "- CARDINAL: 824, 50, 1, 3\n",
            "- PERSON: Bid DorzoIamizvm, Steve Jo jason\n",
            "\n",
            "\n",
            "=== ü§ñ Requesting AI suggestions... ===\n",
            "\n",
            "üì• Please enter your instruction or prompt for Gemini (e.g., 'Fix grammar and summarize'):\n",
            "üìù Your Prompt: give me all the medicines names with suitable dosages\n",
            "\n",
            "=== üí° AI Suggestions ===\n",
            "Based on the OCR-extracted text, here are the medications and dosages mentioned:\n",
            "\n",
            "*   **Betaine:** 100mg 3 times daily (TID)\n",
            "*   **Dorzolamide:** 1 drop 2 times daily (BID)\n",
            "*   **Cimetidine:** 50mg 2 times daily (BID)\n",
            "*   **Oxprenolol:** 50mg once a day (QD)\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **OCR Accuracy:** The text was extracted using OCR, which is prone to errors. There might be mistakes in the medication names or dosages.\n",
            "*   **Incomplete Information:** This is a very limited snippet of data. A complete prescription would include the route of administration (e.g., oral, topical), frequency, duration, and other instructions.\n",
            "*   **Individual Needs:** Dosages are highly individualized and depend on factors like age, weight, kidney function, and other health conditions.\n",
            "\n",
            "**Disclaimer:** This information is for informational purposes only and should NOT be considered medical advice. Always consult a qualified healthcare professional for diagnosis and treatment. Do not make any changes to your medication regimen without consulting your doctor.\n",
            "\n",
            "\n",
            "‚¨áÔ∏è Downloading PDF...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_97956f53-383b-4319-a0ff-54552eceb436\", \"processed_document_with_nlp.pdf\", 2996)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Processing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7Qf7QrcS_ec"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUY-lYfL7Gos",
        "outputId": "cb864c21-d2b8-4b2a-96c7-acf272aac4bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.35.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "Installing collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.44.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HP0ShTrq7KTa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}